version: "3"
services:

  postgres:
    image: postgres
    restart: always
    ports:
      - 5432:5432
    environment:
      - POSTGRES_USER
      - POSTGRES_PASSWORD
      - POSTGRES_DB
      - POSTGRES_PORT

  # Open Telemetry
  #
  # The Otel Collector and Otel Agent are actually the same thing,
  # the only difference is that the agent is configued to be 'lighter'
  # acting as a proxy that runs alongside the host application, forwarding
  # the data through to the actual collector (external to the application host),
  # which then does most of the heavy lifting
  #
  # An agent would be deployed on the same host
  # The collector would be at a cluster/regional level

  # Collector
  otel-collector:
    image: otel/opentelemetry-collector-contrib
    command: ["--config=/etc/otel-collector-config.yaml", "${OTELCOL_ARGS}"]
    volumes:
      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml
    ports:
      - "1888:1888" # pprof extension
      - "8888:8888" # Prometheus metrics exposed by the collector
      - "8889:8889" # Prometheus exporter metrics
      - "13133:13133" # health_check extension
      - "55678" # OpenCensus receiver
      - "55670:55679" # zpages extension
    depends_on:
      - jaeger
      - zipkin
      - prometheus

  # Agent
  otel-agent:
    image: otel/opentelemetry-collector-contrib
    command: ["--config=/etc/otel-agent-config.yaml", "${OTELCOL_ARGS}"]
    volumes:
      - ./otel-agent-config.yaml:/etc/otel-agent-config.yaml
    ports:
      - "1777:1777" # pprof extension
      - "4317:4317"
      - "8887:8888" # Prometheus metrics exposed by the agent
      - "14268" # Jaeger receiver
      - "55678" # OpenCensus receiver
      - "55679:55679" # zpages extension
      - "13133" # health_check
    depends_on:
      - otel-collector

  # Example OTEL compatible backends

  prometheus:
    container_name: prometheus
    image: prom/prometheus
    volumes:
      - ./prometheus.yaml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"

  zipkin:
    image: openzipkin/zipkin
    ports:
      - "9411:9411"

  jaeger:
    image: jaegertracing/all-in-one
    restart: always
    ports:
      - "5775:5775/udp"
      - "6831:6831/udp"
      - "6832:6832/udp"
      - "5778:5778"
      - "16686:16686"
      - "14268:14268"

  apm-server:
    image: docker.elastic.co/apm/apm-server:7.10.2
    depends_on:
      elasticsearch:
        condition: service_healthy
      kibana:
        condition: service_healthy
    cap_add: ["CHOWN", "DAC_OVERRIDE", "SETGID", "SETUID"]
    cap_drop: ["ALL"]
    ports:
    - 8200:8200
    command: >
       apm-server -e
         -E apm-server.rum.enabled=true
         -E setup.kibana.host=kibana:5601
         -E setup.template.settings.index.number_of_replicas=0
         -E apm-server.kibana.enabled=true
         -E apm-server.kibana.host=kibana:5601
         -E output.elasticsearch.hosts=["elasticsearch:9200"]
    healthcheck:
      interval: 10s
      retries: 12
      test: curl --write-out 'HTTP %{http_code}' --fail --silent --output /dev/null http://localhost:8200/

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.10.2
    environment:
    - bootstrap.memory_lock=true
    - cluster.name=docker-cluster
    - cluster.routing.allocation.disk.threshold_enabled=false
    - discovery.type=single-node
    - ES_JAVA_OPTS=-XX:UseAVX=2 -Xms1g -Xmx1g
    ulimits:
      memlock:
        hard: -1
        soft: -1
    volumes:
    - esdata:/usr/share/elasticsearch/data
    ports:
    - 9200:9200
    healthcheck:
      interval: 20s
      retries: 10
      test: curl -s http://localhost:9200/_cluster/health | grep -vq '"status":"red"'

  kibana:
    image: docker.elastic.co/kibana/kibana:7.10.2
    depends_on:
      elasticsearch:
        condition: service_healthy
    environment:
      ELASTICSEARCH_URL: http://elasticsearch:9200
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
    ports:
    - 5601:5601
    healthcheck:
      interval: 10s
      retries: 20
      test: curl --write-out 'HTTP %{http_code}' --fail --silent --output /dev/null http://localhost:5601/api/status

volumes:
  esdata:
    driver: local
